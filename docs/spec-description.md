# Ralph Loop Spec Description

A Ralph Loop is a bash loop that iteratively runs an AI agent to implement a specification via an implementation plan. Each iteration is stateless: the agent re-reads the spec from scratch, selects the next priority from the plan, and implements it. Because no memory carries over between iterations, the spec is the only persistent source of truth — making its clarity and precision the primary determinant of loop reliability.

This document focuses on writing effective specs. For guidance on writing the implementation plan that accompanies a spec, see the companion document [Ralph Loop Plan Description](ralph-loop-plan-description.md).

## Why This Matters

Empirical evidence shows that error compounding is the dominant failure mode for iterative AI systems. Semantic drift — where agent outputs progressively diverge from original intent — affects roughly 50% of multi-agent systems by 600 interactions, and the rate of drift accelerates over time rather than remaining constant. Studies also show that model reliability drops approximately 39% from single-turn to multi-turn settings. These numbers make clear that spec quality is not a nice-to-have but a structural requirement for any loop that expects to run reliably across many iterations.

## The Core Tension

The spec needs to serve as a stable reference point that a looping agent can evaluate against repeatedly without drift. Each iteration, the loop agent reads the spec, picks the top priority from the plan, implements it, and comes back. That means the spec can't be ambiguous in ways that compound across iterations. The specific failure mode is semantic drift: agent outputs progressively diverging from original task intent while remaining syntactically valid. Even a 1% per-token error rate escalates to an 87% error probability by the 200th token — ambiguity in a spec that gets re-read dozens of times doesn't just risk a single bad implementation, it systematically corrupts the trajectory of the entire loop.

## Characteristics of a Great Ralph Loop Spec

- Declarative over procedural. Describe what the system should look like when done, not the steps to get there. The implementation plan carries the procedural weight. The spec is the target state. This separation is what lets the loop pick any priority item and implement it without the spec dictating sequencing. For example: *Before (procedural)* — "First create the ScheduleEntry model, then add validations for start_time and crew_id, then wire up the controller with a POST endpoint." This encodes an implied sequence; if the loop agent picks "wire up the controller" as its plan item, the procedural framing suggests it should already have completed the prior steps. *After (declarative)* — "The ScheduleEntry model validates presence of start_time and crew_id; the API exposes POST /schedule_entries returning 201 on success." The declarative form describes the end state without sequencing, so the loop agent can implement any single plan item — the model, the validations, or the endpoint — independently and verify the result against the spec. A well-written declarative spec should be idempotent: safe to re-read and re-evaluate any number of times without producing different interpretations or side effects. This property enables the loop agent to resume after any interruption — whether a crash, a timeout, or a manual pause — and pick up exactly where it left off, because the spec describes a target state rather than a sequence of actions that may have been partially executed. This mirrors the idempotency requirement in infrastructure-as-code systems, where the same configuration is applied repeatedly to converge on the desired state: running `terraform apply` twice with the same configuration should produce the same infrastructure, and re-reading a declarative spec twice should produce the same implementation intent.
- Independently verifiable assertions. Each requirement in the spec should be something the loop agent can check in isolation after implementing a plan item. "The ScheduleEntry model validates presence of start_time and crew_id" is verifiable. "The scheduling system works well" is not. Think of each line as a potential assertion in a test suite.
- Layered detail with clear boundaries. Structure the spec in concentric rings — a high-level summary of intent, then domain rules, then technical constraints. This lets the agent quickly orient on why before diving into what. When the loop agent picks up "Implement crew assignment validation" from the plan, it should be able to trace from the technical constraint back up to the business rule it serves.
- Explicit non-goals and boundaries. State what's out of scope. In a loop, scope creep is insidious because the agent is making judgment calls each iteration without you reviewing every one. "This spec does NOT cover notification delivery mechanisms" prevents the loop agent from gold-plating (adding unrequested features or polish) a plan item by pulling in adjacent concerns.
- Stable naming and terminology. Define your domain vocabulary once at the top and use it consistently. Treat the glossary as an authoritative, version-controlled artifact — not informal documentation — where every domain term is defined before first use and any term introduced in a spec revision triggers a glossary update check. This is the glossary-as-normative-reference pattern: the glossary is a normative part of the spec, not an appendix. Just as renaming a variable midway through a method prevents compilation, renaming a term midway through a spec prevents the loop agent from "compiling" a coherent implementation. DDD's Ubiquitous Language practice establishes that treating terminology with the same rigor as code prevents the translation-layer errors that compound across iterations. The loop agent is re-reading this spec every iteration — inconsistent terminology across sections produces inconsistent implementations across iterations that you'll have to reconcile later. For example: *Before* — the domain rules refer to a "project," the API constraints call it a "job," and the data model labels it a "work order." Three terms for one concept, scattered across sections, each nudging the loop agent toward a slightly different mental model. *After* — the glossary defines "project" as the canonical term with a note that "job" and "work order" are deprecated synonyms, and every section uses "project" consistently. Both the failure mode and the fix are concrete: the glossary resolves the ambiguity once, and consistent usage prevents it from recurring. The risk here compounds: when instructions use inconsistent terms, models make early incorrect assumptions and compound errors with each subsequent response, rarely recovering without a full context reset. This makes terminology consistency not just a readability concern but a reliability one.
- Terminology evolution governance. Glossaries must evolve as implementation proceeds — new concepts emerge, existing terms sharpen in meaning, and deprecated terminology accumulates. But ungoverned glossary changes introduce the same drift risks the glossary was meant to prevent. Govern terminology refinement with three rules: (1) add new glossary entries rather than overloading existing terms — if "schedule" initially meant a weekly plan but now also refers to a per-shift assignment, create a distinct term like "shift assignment" rather than expanding the definition of "schedule"; (2) explicitly deprecate terms with a pointer to the replacement rather than silently changing definitions — write "~~job~~ → use *project*" so the loop agent and human reviewers can trace the evolution; and (3) annotate which spec revision introduced each terminology change so drift can be audited — when a term's meaning shifts between revision 3 and revision 7, the annotation makes the divergence visible rather than buried in diff history. Automated terminology enforcement — such as linting for deprecated terms or flagging undefined nouns — can catch violations that manual review misses, particularly in long-running loops where the glossary and the spec body may evolve at different rates.
- Pre-writing terminology extraction. Before drafting requirements, conduct a vocabulary extraction pass: list every noun and verb phrase that will appear in the spec, group synonyms and near-synonyms, and identify polysemic terms — words that carry different meanings across contexts (e.g., "schedule" meaning a weekly calendar in the project management domain but a cron expression in the deployment domain). Terms with multiple potential meanings need explicit scoping in the glossary before they appear in requirements, not after a loop agent has already interpreted them inconsistently across iterations. This front-loads consistency rather than retrofitting it after ambiguity is baked in, and it surfaces domain modeling decisions (which concepts are truly distinct vs. which are aliases) that would otherwise emerge piecemeal during implementation.
- Concrete examples for ambiguous rules. Anywhere a business rule has edge cases, include an example. "A crew member cannot be double-booked" is clearer with: "If Juan is assigned to Project A from 7am-3pm, attempting to assign him to Project B from 2pm-5pm should fail with an overlap error." The loop agent doesn't get to ask you clarifying questions mid-run.
- Version-resilient structure. The spec should tolerate the plan items being implemented in a somewhat different order than expected. If implementing item #3 before item #1 would violate an assumption in the spec, that assumption needs to be explicit. Ideally, each section of the spec is as order-independent as possible. A cautionary failure mode from infrastructure-as-code systems applies here: non-deterministic application of declarative specifications can produce race conditions when implicit ordering exists. If section A assumes an entity created in section B without stating the dependency, two plan items touching those sections can be implemented in an order that produces an inconsistent state — one that passes each section's local checks but fails integration. Spec authors should explicitly enumerate data or state dependencies between sections (e.g., "The ScheduleEntry model depends on the Project model existing") so the loop agent can detect conflicts rather than silently producing inconsistent implementations. Making dependencies explicit converts hidden ordering constraints into visible, checkable preconditions.
- Testability hooks. Include enough specificity that each implemented item can be validated — whether that's expected model associations, API response shapes, validation error messages, or database constraints. This gives the loop a way to self-check before moving to the next priority.
- Spec length and decomposition. A spec should stay within a size the loop agent can reliably process in a single read. Effective context utilization drops significantly as document length increases — the agent's ability to hold all constraints in working memory degrades, and the risk of overlooking or misweighting requirements grows with every additional paragraph. When a spec exceeds this practical bound, split it into multiple focused specs, each covering one bounded subsystem. Maintain a short index spec that lists the sub-specs and their relationships (shared domain terms, interface contracts, ordering constraints) so the loop agent can orient itself and navigate between them. Smaller, focused specs also make it easier to version, review, and update individual subsystems without risking unintended side effects in unrelated sections.
- Verification beyond testability hooks. Testability hooks verify individual plan items, but reliable loops need complementary validation patterns that catch subtler failure modes. Three patterns work well together: (1) Self-consistency checks — generate multiple candidate implementations for a plan item and select by consensus; when candidates diverge significantly, it surfaces ambiguous spec language that needs tightening rather than a single interpretation that may be wrong. (2) Cross-verification with external systems — validate outputs against existing APIs, schemas, or test suites rather than relying solely on the loop agent's interpretation of the spec; an external system provides a ground truth the agent cannot rationalize away. (3) Periodic context summarization — have the loop agent emit a summary of its understanding of the spec and progress at defined intervals so that drift between the agent's working model and the spec's actual intent can be caught before it compounds across subsequent iterations.

## The Litmus Test
A great Ralph Loop spec passes this test: if you handed it to a developer with zero context along with any single item from the implementation plan, could they implement that item correctly without asking a question? If yes, Claude Code can loop on it reliably.

The spec is the constitution; the plan is the legislative agenda. Keep them cleanly separated and the loop stays coherent across many iterations.

## Known Limitations

Spec quality alone is insufficient for reliable long-running loops. Even a well-written spec cannot prevent all forms of drift and degradation over many iterations. Effective iterative systems also benefit from monitoring (detecting when outputs begin diverging from spec intent), reset mechanisms (periodically re-grounding the agent by summarizing progress and clearing accumulated context), and architectural safeguards (such as checkpoints, rollback capabilities, and human review gates at defined intervals). The spec characteristics described above raise the floor for loop reliability, but production systems should layer additional defenses on top of them.

Long-running loops also benefit from periodic spec-vs-implementation audits — structured checkpoints where the agent re-reads the entire spec and flags any implemented behavior that no longer matches the declared target state. State drift can be subtle: an early iteration might implement a validation rule correctly, but a later iteration working on a related feature might inadvertently weaken or override it without either iteration detecting the conflict. The lag between drift occurring and drift being detected is itself a risk — the longer the detection interval, the more compounded the divergence, and the harder it becomes to trace the root cause back to the specific iteration that introduced it. Shorter audit intervals catch drift earlier but add overhead; the right frequency depends on the spec's complexity and the loop's iteration rate.
